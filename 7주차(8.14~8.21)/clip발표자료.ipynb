{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53de43ea-9f0b-4bcd-a3dd-a06cc8a82111",
   "metadata": {},
   "source": [
    "# CLIP (Contrastive Language-Image Pre-training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d0b5d3-1928-49b7-8e8a-00c5be99f141",
   "metadata": {},
   "source": [
    "## CLIP은 Text와 Image간의 관계성을 모델링한 연구\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca9eac0-048e-42d6-8c6c-b84e6dac9e5a",
   "metadata": {},
   "source": [
    "# *1.탄생배경*\n",
    "\n",
    "### (1)fine-tuning 없이 새로운 downstream task에 적용하기 어려움= 모델의 일반화↓\n",
    "\n",
    "#### (2)새로운 downstream task에 적합한 다량의 이미지와 레이블링 작업을 요함\n",
    "#### 이미지 수집 및 정답 레이블 생성에 많은 인력과 비용이 요구\n",
    "\n",
    "### (3)벤치마크 데이터셋 성능과 실제 현실에서 수집한 데이터셋 성능과는 차이 존재\n",
    "##### 벤치마크 데이터셋에 최적화되어 그 외 데이터셋에서는 저조한 성능을 보임 = 모델의 강건성↓\n",
    "# ↓\n",
    "# CLIP (Contrastive Language-Image Pre-training)\n",
    "</br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12a3e2c-83d4-4ff7-b4f9-249fee4957f1",
   "metadata": {},
   "source": [
    "# *2.Contrastive Learning*\n",
    "\n",
    "\n",
    "#### (1).Contrastive Learning: 레이블링 없이 학습하는 Self-supervised Learning 방법론 중에 한가지 방법 \n",
    "#### (2).특정 입력을 Embedding Network를 통해 임베딩 공간으로 이동 \n",
    "#### (3).같은 class라면 임베딩 값의 거리를 최소화(d+)하고, \n",
    "#### 다른 class라면 임베딩 값의 거리를 최대화 (d-) 되도록 embedding network를 학습하는 방법\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4920b0-1be8-4ef7-854c-b341f87c2dca",
   "metadata": {},
   "source": [
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F0aSpp%2FbtsCAMDq7AD%2FqW3TOmpOXZD5KX2XR9Db0k%2Fimg.png\" width=\"800px\" height=\"500px\"\n",
    "title=\"contrastive learning\" alt=\"clip1\"></img><br/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c59a0e-6b3c-4793-824c-4e19e92b5168",
   "metadata": {},
   "source": [
    "# *3.CLIP (Contrastive Language-Image Pre-training)*\n",
    "\r",
    "#### \n",
    "인터넷에서 4억개 이미지와 해당 이미지에 대한 설명 Text를 pair로 두고 학습데이터셋으로 구한다.\n",
    "#####  각각을 인코더로 임베딩하여 같은 pair에 대해 거리를 가깝게하고 다른 pair에 대해 거리가 멀어지도록 텍스트/이미지 인코더를 학습킨니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9abb58-39f7-4192-8650-0e92e5972cff",
   "metadata": {},
   "source": [
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FblfEVH%2FbtsCELjEMgl%2Fn5QtiEQCnfXvhlQBRefM9k%2Fimg.png\" width=\"800px\" height=\"500px\"\n",
    "title=\"contrastive learning\" alt=\"clip1\"></img><br/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b450f787-ca72-4539-a51a-aa8b5abb6035",
   "metadata": {},
   "source": [
    "#### 학습된 인코더로 Test 이미지와 Test 텍스트 간의 유사도를 계산 할 수 있다. \n",
    "#### 이미지와 Text의 관계를 사용하는 다양한 Task에 적용이 가능한데, Image Classification을 할 경우, \n",
    "#### 단어 보다 문장 형태인 a photo of {object}로 입력값으로 주면 더 성능이 좋다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eff1383-2aaa-42aa-9610-1b214b940109",
   "metadata": {},
   "source": [
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FJAYXw%2FbtsCANhZatN%2Fq2mk2kgUULo8ICSc5dWvBk%2Fimg.png\" width=\"800px\" height=\"500px\"\n",
    "title=\"contrastive learning\" alt=\"clip1\"></img><br/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f1fbf2-f3ce-4967-adf1-ecf548483baa",
   "metadata": {},
   "source": [
    "#### 학습시에는 이미지와 텍스트간의 cosine similiary를 계산하고, 같은 Pair간에는 유사도를 최대화하고 다른 Pair간에는 최소화하도록 Cross Entropy Loss를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631e89f-f3f7-48dc-8b5b-257bfb1dfc32",
   "metadata": {},
   "source": [
    "# *4.Contrastive Learning Loss*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdecf77-e256-4fbb-9f4e-baf6adcf77e7",
   "metadata": {},
   "source": [
    "### Contrastive Loss\n",
    "\n",
    "#### 주어진 xi, xj를 encoder(f)로 임베딩된 값간의 거리를 구하여, 같은 class(yi=yj)일 때 그 거리 만큼 loss로 가지고 \n",
    "#### 다른 class일 경우 가까울수록 loss가 커지도록 Loss가 설계되었다\n",
    "#### 다른 class이면서 두 임베딩값의 거리가 너무 멀 경우(=margin m보다 클경우) Loss가 너무 커지지 않도록 Loss가 0이 되는게 특징이다.다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b21328-2541-4c34-8efa-0430f20f0460",
   "metadata": {},
   "source": [
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbIJnOh%2FbtsCzRZfpiv%2FfvxUYO4X94LGGH0ZnnbQ90%2Fimg.png\" width=\"800px\" height=\"500px\"\n",
    "title=\"contrastive learning\" alt=\"clip1\"></img><br/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec0905c-5604-42fd-b936-d7562c0b5cff",
   "metadata": {},
   "source": [
    "### Triplet Loss\r\n",
    "#### \r\n",
    "위의 Loss는 2개 입력에 대한 관계로 Loss를 습니다. Triplet Loss는 3개 입력에 대한 관계로 나타진집니다. 기준 입력 Anchor(x)가 있고, Anchor와 같은 class입력 Positive(x+), 다른 Class입력 Negavie(x-)이 있을 때, 'x와x+의 거리'가 'x와x-의 거리'보다 상대적으로 가까워지도록 만준줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb3c79c-c0ee-4173-a1b1-92d00d2a0b70",
   "metadata": {},
   "source": [
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbijHfk%2FbtsCBKSXMUO%2Fnc0jhEbOjIjUVwUTRRXId1%2Fimg.png\" width=\"800px\" height=\"500px\"\n",
    "title=\"contrastive learning\" alt=\"clip1\"></img><br/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae806b9-50dc-4a21-bfd9-caa4ee898030",
   "metadata": {},
   "source": [
    "### (N+1)-Tuplet Loss\r\n",
    "#### \r\n",
    "Triplet Loss에서는 Anchor(x), Positive(x+), Negative(x-) 3가지 입력으로 구성되었다면#### ,\r\n",
    "(N+1)-Tuplet Loss에서는 Anchor(x)1개, Positive(x+) 1개, Negative(x-)가 N개 있는 입력으로 Loss를 구된 됩니\n",
    "#### 다. f+는 x+를 인베딩한 값, fi는 x-를 임베딩이값입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8543829e-4199-4444-96ce-18ee135e2e46",
   "metadata": {},
   "source": [
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F5N8OQ%2FbtsCyJglPiE%2FzZKkmVVW6qpPB20nq8gtT1%2Fimg.png\" width=\"800px\" height=\"500px\"\n",
    "title=\"contrastive learning\" alt=\"clip1\"></img><br/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d111dd-d8be-45ab-b95c-f17bc7225528",
   "metadata": {},
   "source": [
    "# *5.clip vs image captioning*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b537850-160b-4bbd-a1ac-53ea58dcd25e",
   "metadata": {},
   "source": [
    "<img src=\"https://inforience.net/wp-content/uploads/2021/02/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA-2021-02-09-%E1%84%8B%E1%85%A9%E1%84%8C%E1%85%A5%E1%86%AB-10.12.23.png\" width=\"800px\" height=\"500px\"\n",
    "title=\"clip\" alt=\"vs\"></img><br/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa436143-7376-4ca1-ace3-231eae77282a",
   "metadata": {},
   "source": [
    "### 27개 데이터셋에 대한 평균적인 인식 성능을 보여주고 있다. CLIP 모델(CLIP-ViT)이 다른 모든 모델들보다 높은 성능을 보여주고 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4fad5d-8212-4f2b-bd1b-bc9fa8d6d21b",
   "metadata": {},
   "source": [
    "<img src=\"https://inforience.net/wp-content/uploads/2021/02/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA-2021-02-09-%E1%84%8B%E1%85%A9%E1%84%8C%E1%85%A5%E1%86%AB-10.07.59.png\" width=\"800px\" height=\"500px\"\n",
    "title=\"clip\" alt=\"vs\"></img><br/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da81daf-d978-4651-ac65-d7e09d053f98",
   "metadata": {},
   "source": [
    "#### ImageNet, Food101, SUN397 등의 데이터셋을 포함한 총 27개의 데이터셋에 대해서 zero-shot 인식 과정을 수행하여 \n",
    "#### ResNet-50을 이용한 fully supervised learning을 수행한 결과와 성능을 비교하였다. \n",
    "#### 놀랍게도 16개 데이터셋에 대해서 CLIP 모델을 이용한 zero-shot learning이 supervised learning 보다 높은 성능을 보였다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41c8e05-8c89-48ae-880b-3aeac41f0c74",
   "metadata": {},
   "source": [
    "<img src=\"https://jiho-ml.com/content/images/2021/08/clip_performance2.png\" width=\"800px\" height=\"500px\"\n",
    "title=\"contrastive learning\" alt=\"clip1\"></img><br/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0825d5-e7e2-4254-84e6-ca2f18f7458a",
   "metadata": {},
   "source": [
    "#### CLIP은 *domain/distribution shift에 강하다. \n",
    "#### 제품 사진으로 찍은 바나나로 학습된 모델이 그림 스케치나 일상 사진으로 찍힌 바나나 사진을 잘 분류하지 못하는 반면 CLIP은 이러한 경우에도 강합니다.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
